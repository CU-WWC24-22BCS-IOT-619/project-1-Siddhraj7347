# -*- coding: utf-8 -*-
"""Gemini AI trailer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qDfR53BjuX2t_DICnc6mVgnOuUJEsgZY

# <h1 align="center">AI Trailer </h1>


With the "AI Trailer" project, you will learn how to generate customized trailers for longer video files. You can use it for movies, talks, content creation, or any other cool use case that may want to try.

# Required dependencies

# Dependencies
"""

import os
import re
import cv2
import math
import time
import shutil
import librosa
import datetime
import requests
import itertools
import numpy as np
import google.generativeai as genai
from PIL import Image
from TTS.api import TTS
from pathlib import Path
from sentence_transformers import SentenceTransformer, util
from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip, concatenate_videoclips
from typing import Union
from google.colab import userdata
from IPython.display import Video
from pytubefix import YouTube

"""# Parameters

Here we will define all the required parameters to run our project. I defined some good defaults here to create a single trailer, but feel free to experiment with them to better suit your needs.
"""

configs = {
    "project_name": "natural_history_museum_washington_dc",
    "video_path": "videos/natural_history_museum_washington_dc.mp4",
    "trailer_creation": {
        "model_id": 'gemini-1.5-pro-002',
        "cache": True,
        "cache_ttl": 60, # in minutes
        "generation_config": {
            "temperature": 1,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192,
            "response_mime_type": "text/plain",
        }
    },
    "trailer_improvement": {
        "model_id": 'gemini-1.5-flash-8b',
        "generation_config": {
            "temperature": 1,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192,
            "response_mime_type": "text/plain",
        }
    },
    "voice": {
        "model_id": 'tts_models/multilingual/multi-dataset/xtts_v2',
        "device": 'cuda',
        "reference_voice_path": 'voices/sample_voice.wav',
        "tts_language": 'en',
        "n_audios": 1,
    },
    "frame_sampling": {
        "n_frames": 500,
    },
    "frame_ranking": {
        "model_id": 'clip-ViT-L-14',
        "device": 'cuda',
        "n_retrieved_images": 1,
        "similarity_batch_size": 128,
    },
    "clip": {
        "min_clip_len": 3,
    },
    "audio_clip": {
        "clip_volume": 0,
        "voice_volume": 1.0,
    },
}

"""# Setup"""

PROJECT_DIR = Path(f"{configs['project_name']}")
TRAILER_PATH = PROJECT_DIR / "trailer.txt"
FRAMES_DIR = PROJECT_DIR / "frames"
TRAILER_DIR = PROJECT_DIR / "trailers"

genai.configure(api_key=userdata.get('GEMINI_API_KEY'))

"""## Download videos and reference speech

Downloading the videos and the reference audio that we will be using for this demo.
"""

video_dir = "videos"

# Natural History Museum (New Dinosaur Exhibit) Walking Tour in 4K -- Washington, D.C.
video1_url = "https://www.youtube.com/watch?v=fdcEKPS6tOQ"
video1_filename = "natural_history_museum_washington_dc"

yt = YouTube(video1_url)
yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download(video_dir, filename=video1_filename)

# Google I/O 2024 Keynote: Sundar Pichai opening remarks
video2_url = "https://www.youtube.com/watch?v=uFroTufv6es"
video1_filename = "google_io_2024_keynote_sundar"

yt = YouTube(video2_url)
yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download(video_dir, filename=video1_filename)

voice_url = "https://github.com/dimitreOliveira/ai_trailer/raw/refs/heads/main/voices/sample_voice.wav"
voice_output_path = "voices/sample_voice.wav"

Path(voice_output_path).parent.mkdir(parents=True, exist_ok=True)

response = requests.get(voice_url)

with open(voice_output_path, 'wb') as f:
    f.write(response.content)

"""# Creating a trailer step-by-step

![](https://github.com/dimitreOliveira/ai_trailer/blob/main/assets/ai_trailer_gemini.jpg?raw=true)

Now we can actually start the trailer creation process, the diagram above outlines all the steps to give us a high-level overview.

Along the cells below, we will go through each one of those steps generating the intermediate artifacts, I also broke down the code required for each step to facilitate the understanding.

## 1) Trailer script creation

The first step is to create the initial draft of the trailer script, we will do that by sending the video file to Gemini and asking it to create the trailer for us.
"""

def build_model(model_id: str, generation_config: dict, cache: bool = False, video_file: Union[genai.types.file_types.File, genai.caching.CachedContent] = None) -> genai.GenerativeModel:
    """Builds an AI Studio model object.

    Args:
        model_id (str): ID of model being used
        generation_config (dict): Configurations used by the model
        cache (bool): If the video file will be cached at AI Studio
        video_file (Union[genai.types.file_types.File, genai.caching.CachedContent]): Video file

    Returns:
        genai.GenerativeModel: AI Studio model
    """
    if cache:
        model = genai.GenerativeModel.from_cached_content(
          cached_content=video_file,
          generation_config=generation_config,
        )
    else:
        model = genai.GenerativeModel(
          model_name=model_id,
          generation_config=generation_config,
        )
    return model


def upload_to_gemini(file_path: str, model_id: str, cache: bool, cache_ttl: int) -> Union[genai.types.file_types.File, genai.caching.CachedContent]:
    """Uploads the given file to Gemini.

    See https://ai.google.dev/gemini-api/docs/prompting_with_media

    Args:
        file_path (str): Path to the file that will be uploaded
        model_id (str): ID of model being used
        cache (bool): If the video file will be cached at AI Studio
        cache_ttl (int): Time duration (in minutes) that the file will be cached

    Returns:
        Union[genai.types.file_types.File, genai.caching.CachedContent]: Uploaded file reference
    """
    # Upload the file using the Files API
    video_file = genai.upload_file(file_path, mime_type="video/mp4")

    # Wait for the file to finish processing
    while video_file.state.name == 'PROCESSING':
        print('Waiting for file to be processed...')
        time.sleep(10)
        video_file = genai.get_file(video_file.name)

    if video_file.state.name == "FAILED":
        raise ValueError(video_file.state.name)

    print(f'File processing complete: {video_file.uri}')

    if cache:
        # Create a cache with a defined TTL
        video_file = genai.caching.CachedContent.create(
            model=model_id,
            display_name=video_file.name, # used to identify the cache
            contents=[video_file],
            ttl=datetime.timedelta(minutes=cache_ttl),
        )
    return video_file


def get_video_trailer(model: genai.GenerativeModel, video_file: Union[genai.types.file_types.File, genai.caching.CachedContent], output_path: Path, cache: bool, prompt: str) -> str:
    """Query Gemini to retrieve the video trailer script.

    Args:
        model (GenerativeModel): Model ID used by AI Studio
        video_file (Union[genai.types.file_types.File, genai.caching.CachedContent]): Video file
        output_path (Path): Path to save the trailer text
        cache (bool): If the video file will be cached at AI Studio
        prompt (str): Prompt used to generate the video trailer

    Returns:
        str: Text containing the video trailer
    """

    prompt_content = []
    if not cache:
        prompt_content.append(video_file)

    prompt_content.append(prompt)
    response = model.generate_content(prompt_content)

    print("Trailer script response metadata:")
    print(response.usage_metadata)

    trailer = response.text.encode('ascii', errors='ignore').decode()
    output_path.write_text(trailer)

    return trailer

print("1. Retrieving the video trailer script \n")

if not TRAILER_PATH.exists():
    TRAILER_PATH.parent.mkdir(parents=True, exist_ok=True)

VIDEO_TRAILER_PROMPT = """
Write a trailer for this video. The trailer must have between 4 and 6 short paragraphs describing the main scenes.
The trailer should be engaging and, by the end, motivate the listeners to watch the full video.
Write only the speech part don't add any other text artifacts like scene descriptions or speaker annotations.
"""

video_file = upload_to_gemini(
    configs['video_path'],
    configs['trailer_creation']['model_id'],
    configs['trailer_creation']['cache'],
    configs['trailer_creation']['cache_ttl'],
)

trailer_model = build_model(
    configs['trailer_creation']['model_id'],
    configs['trailer_creation']['generation_config'],
    configs['trailer_creation']['cache'],
    video_file,
)

trailer = get_video_trailer(
    trailer_model,
    video_file,
    TRAILER_PATH,
    configs['trailer_creation']['cache'],
    VIDEO_TRAILER_PROMPT,
)

print(trailer)

"""## 2) Trailer improvement

Often it is the case that the first trailer script generated is not quite what we want, for this reason, during the second step, we will query Gemini again but now with a specific prompt that improves the first draft. We mainly want to remove some textual artifacts that are often generated by LLMs and make the script look more appealing as a trailer script.
"""

def improve_trailer(model: genai.GenerativeModel, prompt: str, output_path: Path) -> str:
    response = model.generate_content([prompt])
    print("Improved trailer script response metadata:")
    print(response.usage_metadata)

    trailer = response.text.encode('ascii', errors='ignore').decode()
    output_path.write_text(trailer)

    return trailer

print("2. Improving video trailer \n")

VIDEO_TRAILER_IMPROVEMENT_PROMPT = """
Take the text above and extract only the trailer part, write a single version making it more engaging and exciting.
The trailer must have between 4 and 6 short paragraphs with a single sentence each describing the main scenes.
The trailer should be engaging and, by the end, motivate the listeners to watch the full video.
Write only the speech part don't add any other text artifacts like scene descriptions or speaker annotations.
"""

trailer_improvement_model = build_model(
    configs['trailer_improvement']['model_id'],
    configs['trailer_improvement']['generation_config'],
)

improved_trailer = improve_trailer(
    trailer_improvement_model,
    f"{trailer}\n\n{VIDEO_TRAILER_IMPROVEMENT_PROMPT}",
    TRAILER_PATH,
)

print(improved_trailer)

"""## 3) Scene split

During the 3rd step, we will simply split the whole trailer script into multiple scenes, this is important because each scene will represent a video segment that we will retrieve later.
"""

def get_trailer_scenes(trailer: str, project_dir: Path) -> None:
    """Split the script into scenes (scenes).

    Args:
        trailer (str): Trailer text
        project_dir (Path): Main project's path
    """
    scenes = trailer.splitlines()
    scenes = [x.strip() for x in scenes if x != ""]

    # Weird piece of code to clean undesired text artifacts
    scenes_ = []
    for scene in scenes:
        if scene.startswith('Narrator:'):
            scene = scene[len('Narrator:'):].strip()
        elif scene.startswith('Scene:'):
            scene = scene[len('Scene:'):].strip()
        scenes_.append(scene)

    scenes = scenes_

    for idx, scene in enumerate(scenes):
        scene_dir = project_dir / f"scene_{idx+1}"
        scene_path = scene_dir / "scene.txt"

        if scene_dir.exists():
            shutil.rmtree(scene_dir)

        scene_dir.mkdir(parents=True, exist_ok=True)

        scene_path.write_text(scene)

print("3. Generating trailer scenes \n")

get_trailer_scenes(improved_trailer, PROJECT_DIR)

"""## 4) Voice generation

At this step, we will create the voice-overs scene by scene. We are using a TTS model, so the input will be each scene's text and the output will be the audio version as a .wav file that we will overlap with the video later.
"""

def generate_voice(
    model: TTS, text: str, audio_path: str, reference_voice_path: str, language: str
) -> None:
    """_summary_

    Args:
        model (TTS): TTS model used to generate the audios
        text (str): Text that will be voiced
        audio_path (str): Output path to save the generated audio
        reference_voice_path (str): Reference audio file used for voice cloning
        language (str): Language used for the TTS model
    """
    model.tts_to_file(
        text,
        speaker_wav=reference_voice_path,
        language=language,
        file_path=audio_path,
    )


def generate_voices(
    scenes_dir: list[str], model: TTS, n_audios: int, reference_voice_path: str, language: str
) -> None:
    """Generate voice for each trailer scene.

    Args:
        scenes_dir (list[str]): List with the directories for each scene
        model (TTS): TTS model used to generate the audios
        n_audios (int): Number of audio samples created for each text
        reference_voice_path (str): Reference audio file used for voice cloning
        language (str): Language used for the TTS model
    """
    for idx, scene_dir in enumerate(scenes_dir):
        scene_trailer = (scene_dir / "scene.txt").read_text()
        audio_dir = scene_dir / "audios"
        print(f'\t Generating audio for scene {idx+1} with text "{scene_trailer}"')

        if audio_dir.exists():
            shutil.rmtree(audio_dir)

        audio_dir.mkdir(parents=True, exist_ok=True)

        for idx in range(n_audios):
            print(f"\t\t Generating audio {idx+1}")
            voice_path = audio_dir / f"audio_{idx+1}.wav"
            generate_voice(
                model, scene_trailer, str(voice_path), reference_voice_path, language
            )

print("4. Generating voice over \n")

SCENES_DIR = list(PROJECT_DIR.glob("scene_*"))
SCENES_DIR = sorted(
    SCENES_DIR, key=lambda s: int(re.search(r"\d+", str(s)).group())
)  # Natural sort

tts_model = TTS(model_name=configs["voice"]["model_id"]).to(configs["voice"]["device"])

generate_voices(
    SCENES_DIR,
    tts_model,
    configs["voice"]["n_audios"],
    configs["voice"]["reference_voice_path"],
    configs["voice"]["tts_language"],
)

"""## 5) Frame sampling

Here, we will do a simple intermediate step for the retrieval part, we will take multiple frame samples from the video, and later we will use these samples to find which image better represents each scene.
"""

def create_screeshots(video_path: str, n_frames: int, frames_dir: Path) -> None:
    """Take multiple frames from a video file.

    Args:
        video_path (str): Path to the video file
        n_frames (int): Number of frames that will be taken
        frames_dir (Path): Path to the frame directory
    """
    if frames_dir.exists():
        shutil.rmtree(frames_dir)

    frames_dir.mkdir(parents=True, exist_ok=True)

    cam = cv2.VideoCapture(video_path)

    total_frames = int(cam.get(cv2.CAP_PROP_FRAME_COUNT))

    currentframe = 0

    while True:
        ret, frame = cam.read()
        if ret:
            img_path = frames_dir / f"frame_{currentframe}.jpg"
            if currentframe % (total_frames // n_frames) == 0:
                cv2.imwrite(str(img_path), frame)
            currentframe += 1
        else:
            break

    cam.release()
    # cv2.destroyAllWindows()

print("5. Sampling frames \n")

create_screeshots(configs["video_path"], configs["frame_sampling"]["n_frames"], FRAMES_DIR)

"""## 6) Frame ranking

Now, we take the frames sampled at the previous step and, using a multimodal similarity model, find which frame better represents the text from each scene.
"""

def search(
    query: str, model: SentenceTransformer, img_emb: np.ndarray, top_k: int
) -> dict:
    """Search the `top_k` most similar embeddings to a text.

    Args:
        query (str): Scene text used as a similarity reference
        model (SentenceTransformer): Similarity model used to measure similarity
        img_emb (np.ndarray): Image embeddings used as the retrieval source
        top_k (int): Number of images to be retrieved

    Returns:
        dict: Retrieved images with some metadata
    """
    query_emb = model.encode(
        [query[:75]], # Not the ideal place to truncate
        max_length=512,
        truncation=True,
        convert_to_tensor=True,
        show_progress_bar=False,
    )
    hits = util.semantic_search(query_emb, img_emb, top_k=top_k)[0]
    return hits


def get_image_embeddings(
    model: SentenceTransformer, img_filepaths: list[Path], batch_size: int
) -> np.ndarray:
    """Create embeddings from a set of images.

    Args:
        model (SentenceTransformer): Model used to embed the images
        img_filepaths (list[str]): File paths for all images
        batch_size (int): Batch size of images to embed at the same time

    Returns:
        np.ndarray: Image embeddings
    """
    img_emb = model.encode(
        [Image.open(img_filepath) for img_filepath in img_filepaths],
        batch_size=batch_size,
        max_length=512,
        truncation=True,
        convert_to_tensor=True,
        show_progress_bar=True,
    )
    return img_emb


def retrieve_frames(
    img_filepaths: list[Path],
    scenes_dir: Path,
    model: SentenceTransformer,
    img_emb: np.ndarray,
    top_k: int,
) -> None:
    """Retrieve the `top_k` most similar frame images to a scene text.

    Args:
        img_filepaths (list[str]): File paths for all images
        scenes_dir (Path): Path to the scenes directory
        model (SentenceTransformer): Similarity model used to measure similarity
        img_emb (np.ndarray): Image embeddings used as the retrieval source
        top_k (int): Number of images to be retrieved
    """
    for idx, scene_dir in enumerate(scenes_dir):
        print(f"\t Retrieving images for scene {idx+1}")
        scence_path = scene_dir / "scene.txt"
        scene_frames_dir = scene_dir / "frames"

        if scene_frames_dir.exists():
            shutil.rmtree(scene_frames_dir)

        scene_frames_dir.mkdir(parents=True, exist_ok=True)

        scene = scence_path.read_text()
        hits = search(scene, model, img_emb, top_k=top_k)

        for hit in hits:
            img_filepath = img_filepaths[hit["corpus_id"]]
            img_name = img_filepath.name

            shutil.copyfile(img_filepath, f"{scene_frames_dir}/{img_name}")

print("6. Retrieving frames \n")

print(f"\t Loading {configs['frame_ranking']['model_id']} as the similarity model")
similarity_model = SentenceTransformer(
    configs["frame_ranking"]["model_id"],
    device=configs["frame_ranking"]["device"],
)

img_filepaths = list(FRAMES_DIR.glob("*.jpg"))
print(f"\t Retrieving from {len(img_filepaths)} images")
img_emb = get_image_embeddings(
    similarity_model, img_filepaths, configs["frame_ranking"]["similarity_batch_size"]
)

retrieve_frames(
    img_filepaths, SCENES_DIR, similarity_model, img_emb, configs["frame_ranking"]["n_retrieved_images"]
)

"""## 7) Clip creation

The next step after retrieving the best frame for each scene is taking the video segment corresponding to each frame, this is quite simple, we just take a video segment starting from that frame, long enough to overlap with the audio generated at the 4th step.
"""

def get_clip(video: VideoFileClip, scenes_dir: Path, min_clip_len: int) -> None:
    """Create video clips based on individual frames

    Args:
        video (VideoFileClip): Video file source for the clips
        scenes_dir (Path): Path to the scenes directory
        min_clip_len (int): Minimum clip length
    """
    fps = video.fps

    for idx, scene_dir in enumerate(scenes_dir):
        print(f"\t Generating clips for scene {idx+1}")
        clip_dir = scene_dir / "clips"
        audio_filepaths = scene_dir.glob("audios/*.wav")
        frame_paths = scene_dir.glob("frames/*.jpg")

        if clip_dir.exists():
            shutil.rmtree(clip_dir)

        clip_dir.mkdir(parents=True, exist_ok=True)

        for audio_filepath in audio_filepaths:
            audio_filename = audio_filepath.stem
            audio_duration = math.ceil(librosa.get_duration(path=audio_filepath))
            audio_duration = max(min_clip_len, audio_duration)

            for frame_path in frame_paths:
                frame = int(frame_path.stem.split("_")[-1])

                clip_start = frame // fps
                clip_end = min((clip_start + audio_duration), video.duration)

                clip = video.subclip(clip_start, clip_end)

                clip.write_videofile(
                    f"{clip_dir}/clip_{frame}_{audio_filename}.mp4",
                    verbose=False,
                    logger=None,
                )
                # clip.close() # Sometimes the clip is closed before it finished writing

print("7. Creating clip \n")

video = VideoFileClip(configs["video_path"], audio=True)

get_clip(video, SCENES_DIR, configs["clip"]["min_clip_len"])

"""## 8) Audio clip creation

At this stage, we basically have all the artifacts required for the final trailer, all that is left is combining them. Here, we join the video segment from the 7th step and overlap with the audio from the 4th step, at the end, we will have an audio clip for each scene.
"""

def get_audio_clips(scenes_dir: Path, clip_volume: float, voice_volume: float) -> None:
    """Add generated voice to each clip.

    Args:
        scenes_dir (Path): Path to the scenes directory
        clip_volume (float): Volume of the original clip used for the audio clip
        voice_volume (float): Volume of the generated voice used for the audio clip
    """
    for idx, scene_dir in enumerate(scenes_dir):
        print(f'\t Generating audio clips for scene "{idx+1}"')
        clips_dir = scene_dir / "clips"
        audios_dir = scene_dir / "audios"
        audio_clips_dir = scene_dir / "audio_clips"

        if audio_clips_dir.exists():
            shutil.rmtree(audio_clips_dir)

        audio_clips_dir.mkdir(parents=True, exist_ok=True)

        for audio_path in audios_dir.glob("*.wav"):
            audio_name = audio_path.stem
            audio = AudioFileClip(str(audio_path))

            for clip_path in clips_dir.glob(f"*{audio_name}.mp4"):
                clip_name = clip_path.stem
                clip = VideoFileClip(str(clip_path))

                mixed_audio = CompositeAudioClip(
                    [
                        clip.audio.volumex(clip_volume),
                        audio.volumex(voice_volume),
                    ]
                )
                clip.set_audio(mixed_audio).write_videofile(
                    f"{audio_clips_dir}/audio_clip_{clip_name}.mp4",
                    verbose=False,
                    logger=None,
                )

print("8. Creating audio clip \n")

get_audio_clips(
    SCENES_DIR,
    configs["audio_clip"]["clip_volume"],
    configs["audio_clip"]["voice_volume"],
)

"""## 9) Trailer creation

The final step is also quite simple, we just take all the audio clips from the previous step, order them and join, the end file will be the complete video trailer.
"""

def join_clips(clip_combinations: list[tuple[str]], trailer_dir: Path) -> None:
    """Join audio clips to create a trailer.

    Args:
        clip_combinations (list[list[str]]): List of audio clips to be combined
        trailer_dir (Path): Directory save the trailers
    """
    for idx, clip_combination in enumerate(clip_combinations):
        print(f"\t Generating trailer {idx+1}")
        trailer_path = trailer_dir / f"trailer_{idx+1}.mp4"
        clips = [VideoFileClip(str(clip_path)) for clip_path in clip_combination]
        trailer = concatenate_videoclips(clips)
        trailer.write_videofile(str(trailer_path))

print("9. Creating trailer \n")

if TRAILER_DIR.exists():
    shutil.rmtree(TRAILER_DIR)

TRAILER_DIR.mkdir(parents=True, exist_ok=True)

audio_clips = [list(scene_dir.glob("audio_clips/*.mp4")) for scene_dir in SCENES_DIR]
audio_clip_combinations = list(itertools.product(*audio_clips))

join_clips(audio_clip_combinations, TRAILER_DIR)

"""## Final result

Below you can take a look at the end result.
"""

Video(f"/content/{configs['project_name']}/trailers/trailer_1.mp4", embed=True)

"""# Take advantage of the cached file and generate another trailer

When we work with LLMs, one of the most important parameters is the temperature which can be understood (in a simplified way) as how deterministic or "free" the LLM will be to generate its output, for this use case allowing the model to be more "creative" can be a good thing to get cool and interesting trailers, but this can also lead to some bad options, so we are likely going to create multiple trailers.

The issue as you may guess is that working with videos, especially longer ones will be quite expensive, as the number of tokens will easily pass 1 million, luckily Gemini allows us to use [context caching](https://ai.google.dev/gemini-api/docs/caching?lang=python), this way we can cache the video file and reuse it for every new generation, this will make our requests be much cheaper and faster compared if we needed to send the video again every time.

Since we have cached the video, we can generate another trailer quite easily.

Below, I have packed all the code above into a single function.
"""

def generate_trailer(configs, video_file, trailer_model, trailer_improvement_model, tts_model, similarity_model, video_trailer_prompt, video_trailer_improvement_prompt):
    PROJECT_DIR = Path(f"{configs['project_name']}")
    TRAILER_PATH = PROJECT_DIR / "trailer.txt"
    FRAMES_DIR = PROJECT_DIR / "frames"
    TRAILER_DIR = PROJECT_DIR / "trailers"

    print("1. Retrieving video trailer \n")

    if not TRAILER_PATH.exists():
        TRAILER_PATH.parent.mkdir(parents=True, exist_ok=True)

    trailer = get_video_trailer(
        trailer_model,
        video_file,
        TRAILER_PATH,
        configs['trailer_creation']['cache'],
        video_trailer_prompt,
    )

    print("2. Improving video trailer \n")

    improved_trailer = improve_trailer(
        trailer_improvement_model,
        f"{trailer}\n\n{video_trailer_improvement_prompt}",
        TRAILER_PATH,
    )

    print("3. Generating trailer scenes \n")

    get_trailer_scenes(improved_trailer, PROJECT_DIR)

    print("4. Generating voice over \n")

    SCENES_DIR = list(PROJECT_DIR.glob("scene_*"))
    SCENES_DIR = sorted(
        SCENES_DIR, key=lambda s: int(re.search(r"\d+", str(s)).group())
    )  # Natural sort

    generate_voices(
        SCENES_DIR,
        tts_model,
        configs["voice"]["n_audios"],
        configs["voice"]["reference_voice_path"],
        configs["voice"]["tts_language"],
    )

    print("5. Sampling frames \n")

    create_screeshots(configs["video_path"], configs["frame_sampling"]["n_frames"], FRAMES_DIR)

    print("6. Retrieving frames \n")

    img_filepaths = list(FRAMES_DIR.glob("*.jpg"))
    print(f"\t Retrieving from {len(img_filepaths)} images")
    img_emb = get_image_embeddings(
        similarity_model, img_filepaths, configs["frame_ranking"]["similarity_batch_size"]
    )

    retrieve_frames(
        img_filepaths, SCENES_DIR, similarity_model, img_emb, configs["frame_ranking"]["n_retrieved_images"]
    )

    print("7. Creating clip \n")

    video = VideoFileClip(configs["video_path"], audio=True)

    get_clip(video, SCENES_DIR, configs["clip"]["min_clip_len"])

    print("8. Creating audio clip \n")

    get_audio_clips(
        SCENES_DIR,
        configs["audio_clip"]["clip_volume"],
        configs["audio_clip"]["voice_volume"],
    )

    print("9. Creating trailer \n")

    if TRAILER_DIR.exists():
        shutil.rmtree(TRAILER_DIR)

    TRAILER_DIR.mkdir(parents=True, exist_ok=True)

    audio_clips = [list(scene_dir.glob("audio_clips/*.mp4")) for scene_dir in SCENES_DIR]
    audio_clip_combinations = list(itertools.product(*audio_clips))

    join_clips(audio_clip_combinations, TRAILER_DIR)

# I will keep all the parameters the same as before, so let's only change the project name.
configs["project_name"] = "natural_history_museum_washington_dc_2nd.mp4"

generate_trailer(
    configs,
    video_file,
    trailer_model,
    trailer_improvement_model,
    tts_model,
    similarity_model,
    VIDEO_TRAILER_PROMPT,
    VIDEO_TRAILER_IMPROVEMENT_PROMPT,
)

"""## Second trailer attempt"""

Video(f"/content/{configs['project_name']}/trailers/trailer_1.mp4", embed=True)

"""# Trying with a different kind of video

Above, we have tried this project on a very specific video to demonstrate the concept, ideally, our trailer would capture the most interesting halls of the museum, the images would correspond to the hall's walkthrough and the trailer script would be engaging enough so that you want to watch the full video. But this project is flexible enough that you can use it for many other types of videos.

To give a second example, we will generate a trailer for a completely different kind of video, we will use Sundar Pichai's keynote presentation at Google IO 2024, where many new features from Gemini were presented.
"""

# Again, I am keeping the same parameters as before, so let's only change the project name and source video.
configs["project_name"] = "google_io_2024_keynote"
configs["video_path"] = "videos/google_io_2024_keynote_sundar.mp4"

# This time, I won't use context caching as I am only running inference once.
configs['trailer_creation']['cache'] = False

video_file = upload_to_gemini(
    configs['video_path'],
    configs['trailer_creation']['model_id'],
    configs['trailer_creation']['cache'],
    configs['trailer_creation']['cache_ttl'],
)

# I only need to redefine the 1st trailer generation model, the rest is the same.
trailer_model = build_model(
    configs['trailer_creation']['model_id'],
    configs['trailer_creation']['generation_config'],
    configs['trailer_creation']['cache'],
    video_file,
)

generate_trailer(
    configs,
    video_file,
    trailer_model,
    trailer_improvement_model,
    tts_model,
    similarity_model,
    VIDEO_TRAILER_PROMPT,
    VIDEO_TRAILER_IMPROVEMENT_PROMPT,
)

"""## Final result"""

Video(f"/content/{configs['project_name']}/trailers/trailer_1.mp4", embed=True)